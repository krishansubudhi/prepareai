{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --pre torch torchvision  --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "# pip install torchvision\n",
    "import torch\n",
    "import transformers\n",
    "from torch import nn\n",
    "torch.__version__\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Fill the [form](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) to get access to llama models\n",
    "2. Create a token in https://huggingface.co/settings/tokens\n",
    "3. Run `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cb5bd9b06e4fe2b68fdd4887925865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/var/folders/p4/w2gf3yz967vdfszsjq06lv1w00xgxj/T/ipykernel_49091/4104286051.py\u001b[0m(2)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 2 \u001b[0;31m\u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m\u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is the meaning of life?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "mps:0 torch.int64\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "pdb.set_trace()\n",
    "\n",
    "input_text = \"What is the meaning of life?\"\n",
    "\n",
    "# Tokenize and move input to the appropriate device (MPS or CPU)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to('mps')\n",
    "\n",
    "print(inputs['input_ids'].device, inputs['input_ids'].dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10, num_beams=1,)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/krishansubudhi/miniconda3/envs/pai/lib/python3.12/site-packages/transformers/__init__.py'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949be27230494f2897ffcf9730c60ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x16919c0e0>\n"
     ]
    }
   ],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling pipeline\n",
      "{'role': 'assistant', 'content': \"Yer lookin' fer a description o'\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "print(\"Calling pipeline\")\n",
    "\n",
    "# This does this\n",
    "# pipeline.tokenizer(pipeline.tokenizer.apply_chat_template(messages, tokenize=False))\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=10,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_fast.PreTrainedTokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipeline.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_input = pipeline.tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>',\n",
       " '<|start_header_id|>',\n",
       " 'system',\n",
       " '<|end_header_id|>',\n",
       " 'ĊĊ',\n",
       " 'Cut',\n",
       " 'ting',\n",
       " 'ĠKnowledge',\n",
       " 'ĠDate',\n",
       " ':',\n",
       " 'ĠDecember',\n",
       " 'Ġ',\n",
       " '202',\n",
       " '3',\n",
       " 'Ċ',\n",
       " 'Today',\n",
       " 'ĠDate',\n",
       " ':',\n",
       " 'Ġ',\n",
       " '26',\n",
       " 'ĠJul',\n",
       " 'Ġ',\n",
       " '202',\n",
       " '4',\n",
       " 'ĊĊ',\n",
       " 'You',\n",
       " 'Ġare',\n",
       " 'Ġa',\n",
       " 'Ġpirate',\n",
       " 'Ġchat',\n",
       " 'bot',\n",
       " 'Ġwho',\n",
       " 'Ġalways',\n",
       " 'Ġresponds',\n",
       " 'Ġin',\n",
       " 'Ġpirate',\n",
       " 'Ġspeak',\n",
       " '!',\n",
       " '<|eot_id|>',\n",
       " '<|start_header_id|>',\n",
       " 'user',\n",
       " '<|end_header_id|>',\n",
       " 'ĊĊ',\n",
       " 'Who',\n",
       " 'Ġare',\n",
       " 'Ġyou',\n",
       " '?',\n",
       " '<|eot_id|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.tokenizer.tokenize(tokenizer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 2675, 527, 264, 55066, 6369, 6465, 889, 2744, 31680, 304, 55066, 6604, 0, 128009, 128006, 882, 128007, 271, 15546, 527, 499, 30, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.tokenizer(tokenizer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000,\n",
       " 128000,\n",
       " 128006,\n",
       " 9125,\n",
       " 128007,\n",
       " 271,\n",
       " 38766,\n",
       " 1303,\n",
       " 33025,\n",
       " 2696,\n",
       " 25,\n",
       " 6790,\n",
       " 220,\n",
       " 2366,\n",
       " 18,\n",
       " 198,\n",
       " 15724,\n",
       " 2696,\n",
       " 25,\n",
       " 220,\n",
       " 1627,\n",
       " 10263,\n",
       " 220,\n",
       " 2366,\n",
       " 19,\n",
       " 271,\n",
       " 2675,\n",
       " 527,\n",
       " 264,\n",
       " 55066,\n",
       " 6369,\n",
       " 6465,\n",
       " 889,\n",
       " 2744,\n",
       " 31680,\n",
       " 304,\n",
       " 55066,\n",
       " 6604,\n",
       " 0,\n",
       " 128009,\n",
       " 128006,\n",
       " 882,\n",
       " 128007,\n",
       " 271,\n",
       " 15546,\n",
       " 527,\n",
       " 499,\n",
       " 30,\n",
       " 128009]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.tokenizer.encode(tokenizer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.tokenizer.decode(pipeline.tokenizer.encode(tokenizer_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 9906]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.tokenizer.encode(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
